{% set name = "omnipkg" %}
{% set version = "2.0.0" %}
package:
  name: {{ name|lower }}
  version: {{ version }}
source:
  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: a87e55ba5a0119a9c56a3add004b8f45ddfa9c678ec4c69b32a4f387807f14d0
  
build:
  number: 0
  noarch: python
  script: "python -m pip install . --no-deps --no-build-isolation -vv"
  entry_points:
    - omnipkg = omnipkg.cli:main
    - 8pkg = omnipkg.cli:main
requirements:
  host:
    - python >=3.7,<3.15
    - pip
    - setuptools >=61.0
  run:
    - python >=3.7,<3.15
    # Conditional logic for packaging based on Python version
    - packaging >=23.0  # [py>=310]
    - packaging >=21.0,<22.0  # [py<310]
    - requests >=2.20
    - authlib >=1.6.5
    - filelock >=3.9
    - tomli  # [py<311]
    # Conditional logic for safety
    - safety >=3.0  # [py>=310 and py<314]
    - aiohttp >=3.13.1
    # Conditional logic for pip-audit
    - pip-audit >=2.6.0  # [py>=314]
    - uv >=0.9.5
test:
  imports:
    - omnipkg
  commands:
    - omnipkg --version
about:
  home: https://github.com/1minds3t/omnipkg
  license: AGPL-3.0-only
  license_family: AGPL
  license_file: LICENSE
  summary: 'A distributed runtime hypervisor for Python enabling concurrent, zero-copy multi-framework orchestration.'
  description: |
  OmniPkg 2.0.0 represents a paradigm shift from package management to a Distributed
    Runtime Architecture. It functions as a local Hypervisor for Python, allowing
    conflicting AI frameworks and hardware drivers to run concurrently in the same
    workflow with near-zero latency.

    This release introduces three critical architectural advancements for ML/AI pipelines:

    1. Universal GPU Inter-Process Communication (IPC):
       Implements a custom, framework-agnostic CUDA IPC protocol using raw ctypes.
       This enables true zero-copy tensor handoffs between isolated processes (e.g.,
       passing data from PyTorch to TensorFlow) with ~1.5ms latency, significantly
       outperforming standard serialization methods.

    2. Selective Hardware Virtualization:
       Features dynamic LD_LIBRARY_PATH injection at the worker level. This allows
       different worker processes to utilize different CUDA Toolkit versions or
       runtime libraries (e.g., running legacy models on CUDA 11 alongside modern
       models on CUDA 12) simultaneously on the same host machine without conflict.

    3. Elastic Persistent Daemon Architecture:
       Replaces ad-hoc process spawning with a self-healing, elastic worker pool.
       Workers utilize a "clean slate" architecture, morphing into specific framework
       environments on-demand and purging themselves after execution. This reduces
       environment context switching latency from ~2000ms (cold start) to ~60ms (warm).

    OmniPkg 2.0 eliminates the need for complex container orchestration for local
    multi-model inference, providing a unified, high-performance runtime for
    complex scientific and financial modeling workflows.
  doc_url: https://github.com/1minds3t/omnipkg/tree/main/docs
  dev_url: https://github.com/1minds3t
extra:
  recipe-maintainers:
    - 1minds3t
