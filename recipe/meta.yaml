{% set name = "omnipkg" %}
{% set version = "2.0.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 7e220dd9f4832b1eb175034222f495ff2ae92ee9df391e62ca0973219a9d1622

build:
  number: 0
  noarch: python
  script: "python -m pip install . --no-deps --no-build-isolation -vv"
  entry_points:
    - omnipkg = omnipkg.cli:main
    - 8pkg = omnipkg.cli:main

requirements:
  host:
    - python >=3.7,<3.15
    - pip
    - setuptools >=61.0
  run:
    - python >=3.7,<3.15
    - packaging >=23.0  # [py>=310]
    - packaging >=21.0,<22.0  # [py<310]
    - requests >=2.20
    - authlib >=1.6.5
    - filelock >=3.9
    - psutil >=5.9.0
    - tomli  # [py<311]
    - safety >=3.0  # [py>=310 and py<314]
    - aiohttp >=3.13.1
    - pip-audit >=2.6.0  # [py>=314]
    - uv >=0.9.5

test:
  imports:
    - omnipkg
  commands:
    - omnipkg --version

about:
  home: https://github.com/1minds3t/omnipkg
  license: AGPL-3.0-only
  license_family: AGPL
  license_file: LICENSE
  summary: 'A distributed runtime hypervisor for Python enabling concurrent, zero-copy multi-framework orchestration.'
  description: |
    OmniPkg 2.0.0 represents a paradigm shift from package management to a Distributed
    Runtime Architecture. It functions as a local Hypervisor for Python, allowing
    conflicting AI frameworks and hardware drivers to run concurrently in the same
    workflow with near-zero latency.

    This release introduces three critical architectural advancements for ML/AI pipelines:

    1. Universal GPU Inter-Process Communication (IPC):
       Implements a custom, framework-agnostic CUDA IPC protocol using raw ctypes.
       This enables true zero-copy tensor handoffs between isolated processes (e.g.,
       passing data from PyTorch to TensorFlow) with ~1.5ms latency, significantly
       outperforming standard serialization methods.

    2. Selective Hardware Virtualization:
       Features dynamic LD_LIBRARY_PATH injection at the worker level. This allows
       different worker processes to utilize different CUDA Toolkit versions or
       runtime libraries (e.g., running legacy models on CUDA 11 alongside modern
       models on CUDA 12) simultaneously on the same host machine without conflict.

    3. Elastic Persistent Daemon Architecture:
       Replaces ad-hoc process spawning with a self-healing, elastic worker pool.
       Workers utilize a "clean slate" architecture, morphing into specific framework
       environments on-demand and purging themselves after execution. This reduces
       environment context switching latency from ~2000ms (cold start) to ~60ms (warm).

    OmniPkg 2.0 eliminates the need for complex container orchestration for local
    multi-model inference, providing a unified, high-performance runtime for
    complex scientific and financial modeling workflows.

  doc_url: https://github.com/1minds3t/omnipkg/tree/main/docs
  dev_url: https://github.com/1minds3t

extra:
  recipe-maintainers:
    - 1minds3t
